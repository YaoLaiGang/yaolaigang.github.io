<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Dynamic Region-Aware Convolution详解]]></title>
    <url>%2F2020%2F08%2F03%2FDynamic-Region-Aware-Convolution%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[区域感知动态卷积论文笔记 论文地址:https://arxiv.org/pdf/2003.12243.pdf 该论文提出了DRConv。传统的卷积结构由于权值共享，所以只有通过增加网络宽度和深度来增加网络捕获的图像特征，这种方法会浪费大量的算力。本论文提出的DRConv构造出了一个“智能通道过滤器”，且该过滤器即实现了类似local conv 1的不同像素不同权重的效果，又保证了网络存在平移不变性，同时整个网络的复杂度并没有提升，同时该网络在几大任务图像任务 2中的表现都好于传统网络 3。 Introduction 由于现在主流的卷积神经网络都是权值共享的，所以每一层能够抽取的信息是有限的，如果想要增加抽取的信息，只能通过增加网络的宽度 4和深度 5来实现。 这种传统的方式有很多的缺点，比如： 计算量大，计算效率低 这会造成优化困难，网络调试变得十分复杂 为了解决这一问题，一些学者提出了local conv 6 7 这种对每个像素都有不同权重的做法虽然取得了不错的效果，计算复杂度也没有显著的增加，但是它仍然有两个致命的缺点： 由于取消了权重共享，导致整个网络的参数量非常大，占用空间也很高。 由于失去了平移不变性，这使得该网络不能做分类任务。 除此之外，对于不同的样本local conv仍然共享滤波器，这使得该网络对特殊的样本样本不敏感 8。 为了克服上面的问题，作者提出了DRConv，其执行过程如下： 首先，通过标准卷积生成guided feature，根据guided feature将整个图像分成多个区域，卷积核生成模块\(G(X)\)根据输入图片动态生成每个区域对应的卷积核，且在每个区域的卷积核是权重共享的，保证了平移不变性。 DRConv可以根据不同的区域匹配不同的卷积核，由于卷积核是动态生成的，所以它要比local conv参数少很多，同时其计算复杂度和标准卷积基本一致。 为了验证效果，本文对比了当前效果较好的网络结构，并对卷积常见的任务都做了对比。 Related Work Spatial Related Convolution 最早的空间相关的卷积网络是local conv，思路很简单，对于每个像素，均使用独立的非共享的滤波器。这种网络对于不同的区域有不同的特征分布的时候(如人脸识别)有很大的应用空间，比较典型的代表就是DeepeFace和DeepID系列网络 在目标检测方面，R-FCN使用基于区域的全卷积去提取局部表征。 除了上面的两个卷积网络结构外，一些模型则通过改变空间特征来更好地模拟语义变化，STNet(Spatial Transform Networks)可以通过学习来转动特征图，但是它非常难训练。ACU(active convolution unit)没有固定的卷积单元形状，它的形状(偏移量)是在训练中改变的。 Deformable Convolutional Network 进一步使得相对位移更加动态 Dynamic Mechanism 由于网络对数据的依赖性很强，动态机制也应运而生，首先是SKNet ， 该网络通过抽取不同感受野的特征提升了网络的精度。使用Attention机制的SENet 则通过动态给各个通道加权来提升网络的精度。 在动态权重方面，CondConv 通过对多个权重进行动态的线性组合来获取最终的权重。 对于不同的样本调整特殊的卷积核也是动态机制的运用，Deformable Kernels 通过学习卷积核的偏移来对原卷积进行重采样，而不改变输入数据。 NonLocal 通过计算每个像素点和整体像素点的关系来实现完整的感受野操作。 Dynamic Region-Aware Convolution 首先为了对比，先看一下标准卷积的公式: \[Y_{u,v,o}=\sum_{c=1}^C X_{u,v.c} \ast W_c^{(o)} \qquad (u,v) \in S\] 对于标准卷积，定义输入\(X\in \mathbb{R}^{U \times V \times C}\)，空间维度\(S \in \mathbb{R}^{U \times V}\)，输出\(Y \in \mathbb{R}^{U \times V \times O}\)，权重\(W \in \mathbb{R}^C\)，输出的第o个channel的计算如上，\(\ast\)为二维卷积操作。 为了方便理解，附上一张卷积的计算过程图： 然后我们再看看local conv的计算： \[Y_{u,v,o}=\sum_{c=1}^C X_{u,v.c} \ast W_{u,v,c}^{(o)} \qquad (u,v) \in S\] 这里我们定义非共享权重为\(W \in \mathbb{R}^{U \times V \times C}\)，输出的第o个channel公式如上，其中\(W_{u,v,c}^{(o)}\)表示位置\((u,v)\)上的独立非共享卷积核，即卷积在特征图上移动时，每次更换不同的卷积核。 最后我们看一下guided mask的计算公式： \[Y_{u,v,g}=\sum_{c=1}^C X_{u,v.c} \ast W_{t,c}^{(o)} \qquad (u,v) \in S_t\] 结合以上公式，定义guided mask\(M=S_0, \cdots , S_{m-1}\)用来表示空间维度划分的\(m\)个区域，\(M\)根据输入图片的特征进行提取，每个区域\(S_t (t \in [0, m-1])\)仅使用一个共享的卷积核。定义卷积核集\(W = [W_0 , \cdots , W_{m-1}]\)，卷积核\(W_t \in \mathbb{R}^C\)对应于区域\(S_t\)。输出的每个channel的计算如上，即卷积在特征图上移动时，每次根据guided mask更换对应的卷积核。 Learnable guided mask guided mask决定了卷积核在空间维度上的分布情况，其重要性可想而知，为了能适应不同的样本，该模块必须设计成可以通过损失函数进行反向传递而进行优化。下面讨论一下如何对该模块进行反向传递。 首先，guided mask 上的值的计算公式如下： \[M_{u,v}=argmax(F_{u,v}^0 , F_{u,v}^1 , \cdots , F_{u,v}^{m-1})\] 其中，F我们称之为guided feature，M为guided mask，上面的公式表示M上每个位置\((u,v)\)的计算公式，\(F_{u,v}\)表示位置\((u,v)\)上的guided feature向量，其实上面的公式就是指示M个feature map中位置\((u,v)\)上的最大值的下标。 这里有一个问题，就是argmax函数不可导，无法获得梯度，这样就没办法做反向传播，为了解决这一问题，作者用softmax来近似的替代argmax。 forward propagation guided mask的前向传播比较简单： \[\hat{W_{u,v}}=W_{M_{u,v}} \qquad M_{u,v} \in [0, m-1]\] \(M_{u,v}\)表示guided feature在\((u,v)\)位置上的最大下标，\(W_{M_{u,v}}\)是m个卷积核集中的一个。 back propagation 这里我们使用softmax来代替之前的one hot 编码，以方便进行反向传播： \[\hat{F_{u,v}^j} = \frac{e^{F_{u,v}^j }}{\sum_{n=0}^{m-1}e^{F_{u,v}^{n}}} \qquad j \in [0, m-1]\] 定义\(\hat{F}\)来代替guided mask中的one hot表示，如上，在channel维度上做了softmax，以期望\(\hat{F_{u,v}^j}\)近似的接近0和1。 由于对onehot无法求导，我们对上述公式进行反向传播: \[\nabla_{F_{u,v}} \pounds = \hat{F_{u,v}} \odot ( \nabla_{\hat{F_{u,v}}} \pounds - 1&lt;\hat{F_{u,v}} , ( \nabla_{\hat{F_{u,v}}} \pounds&gt;)\] 这个公式看起来有些复杂，其实本质就是softmax反向求导公式，首先softmax的导函数为\(y(1-y)\)。上面公式中，\(\nabla_{\hat{F_{u,v}}} \pounds\) 表示loss对\(\hat{F}\)求导，我们提出这一个公因子就会发现其本质就是softmax求导再乘于loss对\(\hat{F}\)求导。这也正是反向传播的规律。 上图中的反向求导公式就是前向传播中公式的反向传播 Filter generator module 这部份我们主要讨论多个卷积核是如何生成的，如下图： 过程并不复杂，其实就是通过了两个一维卷积最后分成\(m \times C\)个channel，只需要注意第一个一维卷积使用\(sigmoid(\bullet)\)进行激活，而第二个不使用激活函数即可。 Experiments Classification 分类效果对比 Face Recognition 面部识别效果 COCO Object Detection and Segmentation 目标检测效果 Ablation Study 这一部分通过可视化来解释为什么这样做有用，以及使用什么参数比较好。 Visualization of dynamic guided mask 效果图 Different model size 不同模型正确率对比 Different region number 区域数对结果的影响 Different spatial size 空间尺寸对效果的影响 对每个像素都有权重，这样做的好处是可以充分的学习图像的特征↩ 分类、人脸识别、目标检测、语义分割↩ 使用ImageNet数据集，Mobile Net、ShuffleNetV2, etc.↩ 增加每次抽取的信息量↩ 增加抽取信息的次数↩ Gregor, K., LeCun, Y.: Emergence of complex-like cells in a temporal product network with local receptive fields. arXiv preprint arXiv:1006.0448 (2010)↩ Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to humanlevel performance in face verification. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1701{1708 (2014)↩ 比如在面部识别和目标检测种，对于不同姿势或者角度的图像↩]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机-Java内存区域&内存溢出异常]]></title>
    <url>%2F2020%2F07%2F05%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA-Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F-%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[Java的HotSpot虚拟机是如何管理内存的呢？对于c++程序员来说他们是“The King of The Memory”，然而对于Java开发者来说他们全然不知。这一部分就来探讨下Java虚拟机的各个区域，这些区域的作用、服务对象以及可能产生的问题。 运行时数据区域 一图以说明： Java运行数据区域 注：该区域划分是由《Java虚拟机规范》规定。 程序计数器 可以看成当前线程所执行的字节码的行号指示器。是程序控制流的指示器：分支、循环、跳转、异常处理、线程恢复都需要这个指示器。 Java的多线程机制是通过线程轮流切换、分配处理器执行时间实现的，在任何一个时刻一个处理器都会执行一条线程中的指令。 因此为了线程切换到达需要的位置，每条线程都需要一个独立的程序计数机(在上面的图像中也能看到这一点)。我们把这种线程之间互不影响，独立存储的内存称为“线程私有”的内存。 &gt; 注意：如果线程执行的是Java方法，计数器指的就是正在执行的虚拟机字节码指令地址。但是如果是本地方法1，这个计数器为空(Undefinde)，这是惟一一个在虚拟机规范中没有规定任何OutOfMemoryError的区域。 Java虚拟机栈 虚拟机栈描述的是Java方法执行的线程内存模型: 每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧，用来存储方法的信息(局部变量表、操作数栈、动态连接、方法出口等)。 Java虚拟机栈也是线程私有的。在线程将方法从开始运行到完毕的时候，其实就是栈帧从入栈到出栈的过程。 局部变量表 在很多Java教程中都会把Java虚拟机的内存管理描述为堆区或者栈区2, 实际上远比这复杂的多。当然，这里的栈区就可以理解为局部变量表。 局部变量表存放了各种编译时可以知道Java虚拟机的基本数据类型3 、对象引用4 和returnAddress类型5。 上述数据类型都是用局部变量槽表示，除了64位的long和double占有两个槽之外，其余变量均占有一个槽。 局部变量所需空间在编译期就已经完成分配，这里的空间大小都是用槽来衡量。 该内存区域规定了两类异常 如果线程申请的栈申请的深度大于虚拟机所容许的最大深度，报StackOverFlow错误。 如果栈扩展时无法申请到足够的内存，报OutOfMemory错误。6 本地方法栈 本地方法栈和Java虚拟机栈基本一样，只不过Java虚拟机栈服务Java方法，本地方法栈服务本地(native)方法，在HotSpot中甚至直接将两者合二为一了。 本地方法栈的错误类型和Java虚拟机栈一样。 Java堆 和前面的内存结构不同，Java堆是所有线程公有的，在虚拟机启动的时候创建，它也是虚拟机管理的内存中最大的一块。 Java堆的主要作用是存放对象实例7 。因此Java堆也变成了垃圾回收机制所管理的内存区域8。 &gt; 注意：现在的垃圾回收机制主要是基于分代理论设计，因此在关于Java堆的资料中常常出现“新生代”、“老年代”、“永久代”、“Eden空间”、“From Survivor空间”、“To Survivor空间”等名词。不过这些只是垃圾回收的设计风格，不是Java虚拟机规范对Java虚拟机的划分，且现在HotSpot中也存在着不使用分代理论的垃圾回收器。 Java堆的空间分配方式 在空间分配上，Java堆可以在物理上不连续，但是在逻辑上要连续。对于大对象，多数虚拟机也要求了连续的内存空间。 Java堆可能出现的异常 Java堆既可以是固定大小，也可以是可扩展的(当前主流Java虚拟机均是可扩展，通过-Xmx和-Xms确定)。如果在Java堆中空间不足，无法完成实例分配，或者堆无法扩展，会抛出OutOfMemoryError异常。 方法区 方法区和Java堆类似，在《Java虚拟机规范》中将其描述为Java堆的一个逻辑部分，当然其职责是不同的，所以方法区的小名叫做“非堆”，目的就是和Java堆区分。 方法区本身和Java堆一样，也是所有线程公有的。 作用 方法区主要存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 &gt; 虽然叫方法区，但可能叫永久代9更贴切，虽然这样叫是错误的。 对方法区的管理 方法区是约束非常宽松的区域。和Java堆一样，它的内存分配也不需要连续，也是可以选择固定区域或者可扩展。另外，方法区还可以选择不实现垃圾收集。 方法区主要针对常量池和的回收和类型的卸载，当然由于条件非常苛刻(尤其是类型的卸载)，所以别指望该区域能够节省多大的空间。但是这并不表明对该区域进行回收是没有必要的：以前Sun公司公布的Bug列表中，若干严重Bug就是因为HotSpot对该区域未完全回收造成的内存泄漏。 方法区可能出现的错误 当方法区无法满足新的内存分配需求时，报OutOfMemoryError异常。 运行时常量池 运行时常量池是方法区的一部分，所以在图中并没有标注出来。运行时常量池主要在类加载后存放其常量池表，该表存在于Class文件中10，其重要用途是存放编译器生成的各种字面量和符号引用。 特点 运行时常量池有着其他内存区域所不具有的特点： 《Java虚拟机规范》没有对该区域做任何细节要求，Java虚拟机对Class文件的每一部分包括常量池的格式都有严格规定11。不过一般说来运行时常量池还是保存Class文件中描述的符号引用和由符号引用翻译出来的直接引用。 具有动态性，Java语言并不要求常量一定在编译期生成，运行期间也可以将新的常量放入池中，常用的Sting的 intern() 方法便是使用该特性的例子。 常量池可能出现的错误 既然是方法区的一部分，其错误自然和方法区一样，当无法分配新的空间时，虚拟机挥发出OutOfMemoryError的异常。 直接内存 首先要说明的是，直接内存并不在《Java虚拟机规范》中定义，在图中我们也没有看到直接内存。 这一部分内存的出现主要是由于从JDK1.4开始加入了NIO(New Input Output)类。这个类可以使用Native数据库直接分配堆外内存，然后通过Java堆中的一个DirectByBuffer对象作为这块内存的引用直接进行操作。 这样做的目的是为了避免在Java堆和Native数据中来回地粘贴数据，极大的提高了性能。 直接内存出现的错误 直接内存不受Java堆大小的限制，但是仍然会收到机器总的内存和处理器寻址空间的限制。所以当各个内存的和大于物理内存限制的时候(其他区域有-Xmx限制，直接内存不受限制)，会抛出OutOfMemoryError异常。 思维导图 总结一下Java内存区域的总体情况 native关键字定义的方法，非Java语言编写，主要是调用由C或者C++编译生成的高效dll或者so库↩ 这种说法实际上来自于C和C++↩ boolean、byte、char、short、int、float、long、double↩ reference类型，一个对象起始地址的指针或者代表对象的句柄(老虚拟机使用句柄)↩ 指向了一条字节码指令地址，用于函数返回↩ HotSpot不会动态申请栈容量，只有当申请时就失败才会报该错误。而老旧的Classic虚拟机使用动态扩展↩ 《Java虚拟机规范》中描述为：“所有对象实例以及数组都应该在堆上分配。”↩ 在一些资料中也被称为GC堆(garbage collected heap) 不过千万别翻译成垃圾堆↩ 和Java堆一样，分代设计只是一种设计风格，之所以称之为永久代是由于HotSpot使用收集器来管理方法堆而节省代码，但是由于这种方式存在bug，所以到了Java8已经完全废弃了这种做法↩ Class文件还存放有类的版本、字段、方法、接口等描述信息↩ 这里先说明下动态常量池和静态常量池不是一个东西。JVM在执行某个类的时候，必须经过加载、连接、初始化，而连接又包括验证、准备、解析三个阶段。静态常量池用于存放编译期生成的各种字面量和符号引用，而当类加载到内存中后，jvm就会将静态常量池中的内容存放到运行时常量池中。而字符串常量池存的是引用值，其存在于运行时常量池之中。↩]]></content>
      <categories>
        <category>Java虚拟机</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2020%2F05%2F24%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式是编程中常用的技术手段，普通的字符串操作会使得程序难以复用，而使用正则表达式会极大的提高复用率。 匹配符 要用字符来匹配字符，定义一些基本的的匹配符是不可避免的： \d可以匹配一个数字，\w可以匹配一个字母或者数字，\s 表示匹配空格或者Tab等空白符 . 可以匹配任意字符 对于变长匹配，* 表示任意个字符（包括0），+ 表示最少一个字符，? 表示0或者1个字符，{n} 表示匹配n个字符，{n,m} 表示匹配n-m个字符 例题1. 请分析下 \d{3}\s+\d{3,8} 所表示的含义？ 解： \d{3}表示三个数字 \s+ 表示最少一个空格 \d{3,8}表示3-8位的数字 答： 这个正则表达式表示的是一个用空格隔开的电话号码。 精确匹配 上述匹配符有一个致命的问题，比如如果我想匹配一个用空格或者 - 分开的电话号码？似乎难以解决这个问题，如果写两个正则表达式十分不优雅，所以我们又引入了以下几个操作符： []表示了范围，如 [0-9a-zA-Z\_] 匹配一个数字、字母或者下划线 A|B 表示匹配A或者B，| 表示或者匹配，如 (P|p)ython 匹配Python或者python ^ 表示行的开头，如 **^* 表示以数字开头 $ 表示行的结尾，如 $ 表示以数字结尾 注意事项 正则表达式使用的是贪婪匹配，也就是尽可能多的匹配尽量多的字符 例题2. 请写出下面程序的结果： re.match(r'^(\d+?)(0)$', '102300').groups() 答：('102300', '') 例题3：请写出匹配102300最后一个0的正则表达式？ 答：^(\d+?)0$ 也就是说，对于 \d+ 这种贪婪匹配，加上 ? 就会变成非贪婪匹配（尽可能少匹配）]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SKNet笔记]]></title>
    <url>%2F2020%2F05%2F11%2FSKNet%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[SKNet(Selective Kernel Convolution)是ResNext后时代的产物，它和SENET是一对兄弟，其灵感来源于我们的视觉神经在看物体时会有不同的感受野。SKNet在实际的数据中表现的也很好。 Abstract 在经典的ResNext系列网络中，卷积神经网络的感受野是相同的。但是众所周知，我们人类的视觉系统在观察的时候感受野明显不相同(在观察远处物体时明显感受野会增加)，作者根据这一灵感提出了SKNet----不同分支有不同的感受野，并且增加了对这些分支的Attention操作，让网络自己学习选哪一种感受野。 Introduction 在对猫视觉的研究中，猫的视觉神经在对同一物体的感受野是不同的，这可以使得猫搜集到多尺寸的信息，对于这一特性最经典的应用就是InceptionNets，它通过融合3×3、5×5、7×7三种不同的卷积核来提升网络的效果。 在刚开始对神经视觉的实验中，科学家一度认为猫看木棍时有一个固定的感受野，称为CRF(Classic )。后来很多科学家发现，在猫盯着棍子的时候，如果你在其注意力外放一个物体猫的视觉神经仍有反应，这被称为NCRF(Non-classic)。并且，如果持续的刺激NCRF一段时间，猫的CRF也会发生改变。也就是说猫的视觉神经感受野是有自动调节功能的。可惜的是只有InceptionNets系列利用了这一网络，但是它仍然是固定了三种不同的视野，并且只采用了简单地线性相加，这种线性相加很有可能会削弱视觉上的动态调整特性。 基于上面几点，作者提出了SKNet，该网络拥有自适应感受野。由Split、Fuse、Select三部分组成。 SKNet具有如下优点： 网络参数少 识别效果好 感受野自适应(作者通过增大检测目标的尺寸和缩小背景尺寸来观察感受野的变化) Releated Work 2.1 多分枝卷积神经网络 Highway networks, ResNet, shake-shake, FractalNets, Multilevel ResNets, InceptionNets 2.2 分组卷积、深度可分离卷积、扩张卷积 AlexNet, ResNext Xception, MobileNetV1, MobileNetV2, ShuffleNet 2.3 注意力机制 SENet, BAM, CBAM 2.4 动态卷积 Spatial Transform Networks, Deformable Convolutional Networks Methods 3.1 Split 如上图所示，split主要有两种操作： \(对于输入X\subseteq \mathbb{R}^{H^{`} \times W^` \times C^`}有:\) \(\widetilde{F}:X \to \widetilde{U} \subseteq \mathbb{R}^{H^{`} \times W^` \times C^`}\) \(\widehat{F}:X \to \widehat{U} \subseteq \mathbb{R}^{H^{`} \times W^` \times C^`}\) 其中，两方法都是用高效分组卷积或者深度可分离卷积、BatchNormalization和Relu。如果还想要提高效率，可以把5×5卷积替换成3×3和2×2组成的扩张卷积。 3.2 Fuse 对于不同卷积核的选择实际上还是通过门结构进行控制，为此需要获取到所有的信息，然后让网络决定能否通过。Fuse就是为了获取整个信息而做的融合。 \(U = \widehat{U} + \widetilde{U}\) \(s_c = F_{gp}(U_c)=\frac{1}{H×W}\sum_{i=1}^{H}\sum_{j=1}^{W}U_c(i,j)\) \(z = F_{fc}(s) = Relu(BatchNormalization(Ws)) \quad ,W\in\mathbb{R}^{d×c},z\in\mathbb{R}^{d×1}\) 这里的d表示缩减后的维度，一个典型的取值为32 \(d=max(C/r, L) \quad ,c=32\) 3.3 Select 在该阶段对压缩特征矩阵z使用Soft Attention来让网络动态调整其RF \(a_c = \frac{e^{A_cz}}{e^{A_cz}+e^{B_cz}}\quad b_c = \frac{e^{B_cz}}{e^{A_cz}+e^{B_cz}}\) 其中，\(A、B \in \mathbb{R}^{C×d}\)为扩展矩阵，其目的是为了把d×1的矩阵扩展到C×1。下标c表示矩阵中的第c的元素。 最终的特征图V通过上面计算的Attention权重a,b最终加权获得： \(V_c = a_c\cdot \widetilde{U}_c+b_c\cdot \widehat{U} \quad , a_c+b_c=1\) \(V=[V_1, V_2, \cdots,V_C] \quad V_c \in \mathbb{R}^{H×W}\) 网络结构设计 网络结构图如下： 其中，G表示分组卷积每一组的Kernel size，fc表示SENet中两个全连接层的输出维度，M表示分路个数，r前面已经说明，表示特征的压缩系数。 SENet可以插入到其他的网络结构中( MobileNet, Shuf- fleNet)，这里插入到ResNext中是为了做对比。 实验 5.1 ImageNet 分类任务 5.1.1 （上图中的224、320表示图像增强中的图像裁剪） 5.1.2 与最优模型的对比 (上图中的括号表示相对于ResNext的提升) 5.1.3 轻量级模型对比 5.1.4 参数性能表现 5.2 CIFAR 分类任务 为了衡量网络在轻量数据上的表现，使用CIFAR 100分类和10分类数据来衡量网络的性能 5.3 Ablation Studies 这部分研究主要是为了证明SKNet可以自适应的调整RF，仍然使用ImageNet为基本数据集。 5.3.1 扩张系数D和分组数G D和G的累积会直接影响到RF大小，实验里固定了一条路径的系数，调整另一条路径的系数如下标，其中的Resulted Kernel表示等效的卷积核大小。从图中可以发现，即使感受野等效的情况下，使用小卷积核和扩张系数的搭配要比直接使用大卷积核好得多。 5.3.2 不同大小卷积核的结合 通过实验不难发现，当M(路的个数)增加时，识别的误差越来越小；无论使用多少M，使用SK都比不使用好；当使用SK时，从M=2到M=3提升不大，更推荐使用M=2。 5.4 自适应RF的解释 这一组实验的具体方法是使用同一组数据，但使用不同的目标物体和背景的尺度，进而观察Attention的权重系数来解释自适应的RF。 从实验中不难发现，当目标物体变大的时候，Attention Weight也逐渐变大(5×5的权重系数减去3×3的权重系数)，这表示神经网络及时的将感受野调大，这个结果完全符合预期。 总结 SKNet从神经学中的可调节感受野出发，通过Attention设计了一套自适应RF的网络结构，在不提高网络复杂度的情况下有效地提升了神经网络的正确率，并且拥有充足的可解释性。]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树结构]]></title>
    <url>%2F2019%2F12%2F04%2F%E6%A0%91%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[&gt; 树结构是一个程序员必须十分熟悉的结构，它包括了二叉树以及二叉树的各种变种、多叉树等，将树结构推广便是图，而树的前中后序遍历就对应了图的DFS，树的层序遍历对应了图的广度优先遍历。 除了特殊问题之外，大部分的关于树的问题的根本是在树的遍历。为了加快遍历的速度，产生了多种不同结构的树结构。 其中、二叉排序树、平衡二叉树、B树等都是为了针对数据方便查找而对树中节点做了约束。 线索二叉树设计的目的也很巧妙，充分利用了剩余节点来把树连接成一个类似链表的结构，使得树的遍历更加方便。 满二叉树、完全二叉树是树的节点满足某种规律时对树的一种称呼。 扩充二叉树是为了克服单一遍历顺序无法构建一个完整的二叉树(否则只能通过先序+中序、先序+后序来确定一个二叉树)。]]></content>
      <categories>
        <category>data structure</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neighbor Embedding]]></title>
    <url>%2F2019%2F09%2F10%2FNeighbor-Embedding%2F</url>
    <content type="text"><![CDATA[简单的说，Neighbor Embedding是一种降维方法，而且是无监督的，利用的是各个单词的临近关系 Manifold Learning(流形学习) 对于降维问题的思考在于，我们的降维在高维空间和低维空间看到的东西是不同的： 上图很容易看到，蓝色的点距离绿色的点更近、距离红色的点更远。但是如果我们只是简单的压扁，就会造成蓝色的点距离红色的点更近的错觉，这就是降维导致的信息丢失。 我们流形学习的思想就是：拉平这个高维空间，而不是压扁这个高位空间，以防止丢失信息： Locally Linear Embedding(LLE) 这种算法的思路是保持数据中任意一个元素和其临近的K个元素的关系\(w_{i,j}\)保持不变: \(Find\, a\,set\,of\,w_{i,j}\,minimizing\\\sum_{i}\|x^i\,-\,\sum_{j}w_{i,j}x^j\|_2\) Then find the dimension reduction results \(z^i\) and \(z^j\) based on \(w_{i,j}\): 降维后的新元素\(z^i\)和\(z^j\)之间，也应该保持上述高维空间的关系，即\(w_{i,j}\)保持不变: 这种关系就和《长恨歌》中的一句诗句类似: LLE中，K的选择是一个关键要素：K选的少了高维的关系可能无法维系；K选的多了算法的复杂性很大，效果也不好： Laplacian Eigenmaps]]></content>
      <categories>
        <category>Unsupervised Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2019%2F09%2F04%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.基本数值特征 对于刚拿到的数据，先进行缺失值处理 对于离散值，我们常用 LabelEncoder OneHotEncoder来对其进行编码以方便识别。常用的为pandas中的get_dummies()方法，当然sklearn 也有对应的类 对于二值特征:Sklearn中的二值化类是Binarizer，当然Pandas直接判断也很方便 对于多项式特征:也就是多个特征指标的组合(一般支持向量机用的比较多，乘积、平方等等)，使用PolynomialFeatures来实现。 对于binning(连续)特征:可以做离散化，划分区间分成几个类(比如年龄)，可以直接写map()函数映射。比较常用的方法,可以直接试一试，效果不好说。 分位数切分: 四分位，二分位，四分之三分位等切分成离散的方式，常用pandans的quantile()方法，然后用qcut()的方法进行切分 对数变换(COX-BOX): 对于分布是正态分布假设的算法(可以用偏度来判定)，数据本身可能偏度很大，而做了对数变换有可能使得其更接近对数变换。使用numpy.log()即可。COX-BOX是另一种更复杂的对数变换，其目的也是让数据近似于正态分布。 2.日期特征处理 一般的日期转化为dataTime格式，可以直接把年月日拿出来用的数据(apply)。提取出年月日之后可以当成特征，也可以二次加工。比如四季、早晚、前半年、后半年等等。 整体的思路是:前期多提特征，后期再筛选。 3.文本特征处理 文本特征的难度是如何让计算机认识这些文字，文字本身对于计算机来说很难提取出特征 可以使用nltk工具包进行预处理，具体使用应该专门学习 关于语句的编码，常用的有词袋模型(对输入的词进行编码，句子中出现的词表示为1，未出现的词表示为0，其实就是词汇分布表，并包含有词频，sklearn中有对应的CountVectorizer) 4.时间序列的特征挖掘 统计特征 : 最大值(max)，最小值(min)，均值(mean)，中位数(median)，方差(variance)，标准差(standard variance)，偏度(skewness)，峰度(kurtosis) 关于偏度和峰度，它们是如下两个公式： \[\mu\,=\,\frac{1}{T}\sum_{i=1}^{T}x_i\] \[\sigma^{2}\,=\,\sum_{i=1}^{T}\frac{1}{T}(x_i - \mu)^{2}\] \[skewness(X)\,=\,E[(\frac{X\,-\,\mu}{\sigma})^{3}]\,=\,\frac{1}{T}\,\sum_{i=1}^{T}\frac{(x_i\,-\,\mu)^3}{\sigma^{3}}\] \[kurtosis(X)\,=\,E[(\frac{X\,-\,\mu}{\mu})^4]\,=\,\frac{1}{T}\,\sum_{i=1}^{T}\frac{(x_i\,-\,\mu)^4}{\sigma^4}\] 熵特征 熵是衡量数据确定性和不确定性的指标，在相同的方差、均值和中位数的情况下，entropy越大，系统就越混乱。 entropy公式如下: \(entropy(X)\,=\,-\sum_{i=1}^{\infty}\,P\{x\,=\,x_i\}\,ln(P\{x\,=\,x_i\})\) 上面是信息论中基本的熵公式，下面介绍几个在时间序列中运用的熵: 2.1 Binned Entropy 这种做法是把时间序列进行分桶的操作，计算每个桶内的熵，以此来衡量时间序列的集中程度 如果一个时间序列的 Binned Entropy 较大，说明这一段时间序列的取值是较为均匀的分布在\(\,min(X_T), max(X_T)\,\)之间。如果取值较小，说明其取值是集中在某一段上的. \(binned entropy\,=\,-\sum_{k=0}^{min(maxbin,len(X))}\;p_k\,ln(p_k)\,\cdot\,1_{p_k&gt;0}\) 2.2 Approximate Entropy 这个指标可判断这个时间序列是具备某种趋势还是随机出现。AE这种方法是将一位空间中的时间序列提升到高维空间中，通过高维空间中向量的距离或者相似度来判断一维空间中是否存在某种趋势。算法大致如下: step1: 给定两个参数m,r .其中m表示取多长的子片段分析，r表示投射到高位时两向量是否相近的阈值。需要构造的m维向量如下: \(X_1(m)\,=\,(x_1,\cdots,x_m)\in\mathbb{R},\) \(X_i(m)\,=\,(x_i,\cdots,x_{m+i-1})\in\mathbb{R},\) \(X_{N-m+1}(m)\,=\,(x_{N-m+1},\cdots,x_N)\in\mathbb{R}.\) step2: 通过上述m维向量可以计算出哪些向量和\(X_i(m)\)相似: \(C_i^m(r)\,=\,(number\,of\,X_j(m)\,such\,that\,d(X_i(m),X_j(m)\,\leq\,r)/(N-m+1))\) 在这里，d可以选取\(L^1,L^2,L^P,L^\infty\)范数，这里常用\(L^\infty\)范数. step3: 考虑函数: \(\Phi^m(r)\,=\,(N-m+1)^-1\:.\sum_{i=1}^{N-m+1}ln(C_i^m(r))\) step4: Approximate Entropy可定义为: \(ApEn(m,r)\,=\,\Phi^m(r)-\Phi^{m+1}(r)\) remark: m一般取值2或者3，r&gt;0, r需要根据具体的时间序列进行调整 如果某个时间序列有许多很相似或者重复的时间序列，那么他的Approximate Entropy就相对较小，反之就相对较大。 2.3. Sample Entropy 测试]]></content>
      <categories>
        <category>feature engineering</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word Embedding]]></title>
    <url>%2F2019%2F08%2F30%2FWord-Embedding%2F</url>
    <content type="text"><![CDATA[核心问题：如何用一个vector表示单词？ 1-of-N encoding 这是最简单的做法，相当于对所有的文字做了一个onehot编码，也就是用一个长度为N(N表示单词的个数)的向量来唯一的表示一个单词。 1-of-N encoding 但是这种方法无法表达各个单词之间的关系，比如dog和cat都是动物，解决方法是给他们再做分类 即使这样，这不能表达多个类别之间的关系。比如class1和class2之间其实是有关系的，因为动物是可以做跑跳的，显然这种硬分类也无法完全表达信息。 Word Embedding 把每一个Word都project到一个高维空间中去，这里的高维空间要比N低多得多。 上图中各个点的关系可以通过在高位空间中的距离或者其他的指标来表示，也可以根据不同的标准来分类。 产生这种向量是非监督学习，我们只知道输入不知道输出 可以使用auto-encoder吗？ 显然是不可以的，输入的是onehot编码，其实是学不到什么东西的。 如何通过Word Embedding 来学习上下文信息? 虽然机器不懂得蔡英文和马英九，但是根据上下文是可以把蔡英文和马英九归并成一类的。 Count based（Glove Vector） 如果单词X，Y经常在一块出现，V(X)和V(Y)就会很接近。 具体做法是让V(X)和V(Y)的内积接近于X和Y共同出现的次数\(N_{x,y}\) Predition based 这种方法对单词的表示仍然是onehot。该方法会训练一个NN，输入是一个onehot的单词编码，输出是一个概率向量，表示某个单词紧跟着该单词的可能性大小。 具体说来，其实是将上图中训练好的网络的第一层向量（Z）取出来作为该词汇的特征向量。 为什么这种方法会奏效呢？ 中间的隐藏层，需要把同类或者相近的词汇投射到相同的区间。只有这样才能降低最终的loss。这种方法当然自动的考虑了上下文关系。 只用一个单词来预测下一个单词肯定是不太现实， 所以引入了Sharing Parameters的模型 其实就是用前N个词汇来预测下一个词汇 如上图，这里前N个单词的权重是共享的，对应的连接处是相同的。其原因一是为了计算方便，二是为了保证同一个单词在不同位置输入得到的特征向量是相同的(比如，就职前面的蔡英文和马英九不应该因为顺序不同而得到大相径庭的结果)。 上面的公式给了一个等价变换。要得到单词的embedding，在训练完之后，只需要乘以那个W即可得到对应的embeddding。 另外在实际训练的时候，为了保持W相同，应该做到： 这个想法十分的巧妙，对反向传播做了小小的改动。 基本的训练过程 当然变形的训练有好多种，他们的优势都各有千秋： Continues bug of word 之前是考虑上文，这里改成了考虑上下文 Skip-gram 用中间的Word来预测上下文 应用 对于我们训练出来的词向量，还有很多有意思的操作。 比如对两个词向量做减法，就能得到一些规律: \(V(hotter)-V(hot)\,\approx\,V(bigger)-V(big)\) \(V(Rome)-V(Italy)\,\approx\,V(Berlin)-V(Germany)\) \(V(king)-V(queen)\,\approx\,V(uncle)-V(aunt)\) 那么我们的机器就可以推测，罗马的意大利就和柏林的？？一样？ \(Compute\,V(Berlin)-V(Rome)+V(Italy)\) 之后就寻找最接近上述结果的词向量，就能找到答案 我们还可以做多语言的Word Embedding，中英文词汇翻译也可以做到 就像上面一样，我们可以学习一些对应关系，然后就可以进行翻译这种功能了。 除了对文字的Embedding，还可以对图像做 这种方式可以用来做一些分类功能，因为传统的方法无法区分新增加的类别。这种方法即使没有这一类，至少也能区别出来不是已知的类别 我们甚至可以多document做Embedding，最简单的方法就是对文档做词袋，然后用auto-encoder。但是词袋无法考虑语言的顺序，会失去很多的信息。 下面是一些解决办法，需要深入研究可以直接去拜读。]]></content>
      <categories>
        <category>Unsupervised Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记]]></title>
    <url>%2F2019%2F08%2F30%2F%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[闲暇之余，想写篇日记，记录自己的所见所闻. 我的博客已经三个月没有更新东西了， 这个三个月可能发生了一些事情，也可能没有发生什么事情，但是终究还是发生了一些事情的。 &quot;我太难了&quot; &quot;我太难了&quot;， 当收到导师召回的消息时，这句话便脱口而出。纵观我没几天的暑假，这可能是最悲惨的消息了。我的暑假分布大概是这样的：98%的时间去练习科目三并最后准备考驾照，2%的时间去跪棚。整个暑假都在奔波中度过。 那天是台风利奇马来临的日子，刚开始它还是听和蔼的，只是淅淅沥沥的小雨，甚至给人一种 渭城朝雨浥轻尘，客舍青青柳色新 的错觉。我踏着单车，带着一种刚回家的喜悦奔向了科目三的练习地址。 “这不还在市区么，虽然是外环，有地图APP肯定没问题”， 自信满满的我如是道。 开始只是濛濛细雨，周围是一条小河，这种环境让我似乎穿梭在江南的小镇。我顺着怡人的柏油路，一边骑车一边哼着小曲赶往目的地。 “这地图为啥没标出来？”我一阵惊呼，这是到目的地的必经之路--一座桥。不幸的是这座桥似乎正在被修，因为它被封死了。 雨兄似乎也很给力，知道我认不得路的时候顺便增大了雨势。嗯，真的是加量不加价呢。 经过一番挣扎和一顿问路后，我终于曲曲折折的绕到了练车的地方。此种心酸不必多说，比如全身湿透这种基本操作，可以尽情脑补这种囧事。 总而言之，我还是到了，虽然晚了半个小时。 “XXX教练今天休息，你明天再来吧”。 当时我的心里其实是很平静的。“真正的勇士，敢于正视淋漓的大到暴雨，敢于直面被人放鸽子的惨淡人生”，这可能是我当初最好的感受。此时的雨兄可以称得上是大到暴雨了，我的心里也是大到暴雨。 未完待遇]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Set Matrix Zeroes]]></title>
    <url>%2F2019%2F05%2F27%2FSet-Matrix-Zeroes%2F</url>
    <content type="text"><![CDATA[问题描述 给矩阵置0，就是找矩阵中为0的元素，将其同行同列置0,必须是就地算法（in-place） 1. Example 1: - input: [ [1,1,1], [1,0,1], [1,1,1] ] - output: [ [1,0,1], [0,0,0], [1,0,1] ] Example 2: input: [ [0,1,2,0], [3,4,5,2], [1,3,1,5] ] output: [ [0,0,0,0], [0,4,5,0], [0,3,1,0] ] 整体思路 思路1.0版本，朴素想法 最朴素想法，先找为0的坐标，然后将其同行同列置0。这里有个小trick，找到为0的地方不能立即置0，下侧和右侧会导致判断失误。 代码1.0 123456789101112131415161718192021222324252627282930 private void setZeroes(int[][] matrix) &#123; //最朴素想法，先找，然后置0 //注意：置0不能影响后面 HashSet&lt;Integer&gt; row = new HashSet&lt;&gt;(), col = new HashSet&lt;&gt;(); if (matrix.length==0) return; int m = matrix.length, n = matrix[0].length; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (matrix[i][j]==0)&#123;// 填充0 row.add(i); col.add(j); &#125; &#125; &#125;// 置0 for (int i : row) &#123; for (int k = 0; k &lt; n; k++) matrix[i][k] = 0; &#125; for (int j : col) &#123; for (int k = 0; k &lt; m; k++)&#123; matrix[k][j] = 0; &#125; &#125; for (int[] nums : matrix) &#123; System.out.println(Arrays.toString(nums)); &#125; &#125; 思路2.0版本 上个版本肯定是不work的啦，毕竟最朴素想法时空只能打败30%左右的样子。作为一个never setter的人，怎么能容忍这么高的时空复杂度。 上一种方法空间复杂度为O(m*n),我想办法降到O(1)。注意到当检查到matrix[i][j] == 0 ,不能直接所有行 列置0的原因是会影响下侧和右侧的判断。但是上侧和左侧不会影响，故我们不再使用HashMap，直接将matrix[i][0] = matrix[0][j] = 0 ,然后再检查一下行列开头即可。注意如果本来第0行或者第0列就有0，需要用一个flag来记忆一下，然后再判定置0。 代码2.0 123456789101112131415161718192021222324252627282930313233343536373839404142private void setZeroes2(int[][] matrix)&#123; int m = matrix.length, n = matrix[0].length; boolean isCol = false, isRow = false; for (int k = 0; k&lt;m; ++k)&#123; if (matrix[k][0] == 0) &#123; isCol = true; // 第一列应当置0 break; &#125; &#125; for (int k = 0; k&lt;n; ++k)&#123; if (matrix[0][k] == 0)&#123; isRow = true; // 第一行应当置0 break; &#125; &#125; for (int i = 1; i &lt; m; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; if (matrix[i][j]==0)&#123;// 填充0 matrix[i][0] = 0; matrix[0][j] = 0; &#125; &#125; &#125; for (int i = 1; i&lt;m; i++)&#123; if (matrix[i][0] == 0)&#123; for (int j = 1; j&lt;n; ++j) matrix[i][j] = 0; &#125; &#125; for (int j = 1; j&lt;n; j++)&#123; if (matrix[0][j] == 0)&#123; for (int i = 1; i&lt;m; ++i) matrix[i][j] = 0; &#125; &#125; if (isRow) for (int k = 0; k&lt;n; ++k) matrix[0][k] = 0; if (isCol) for (int k = 0; k&lt;m; ++k) matrix[k][0] = 0; for (int[] nums : matrix) &#123; System.out.println(Arrays.toString(nums)); &#125; &#125; 中等题就是这样，解出来比较简单，但是想要拿个top还是比较难的。不管怎样，第二种解法也是top 98%的存在。那么就来个九转大肠鼓励一下自己吧！]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Regular Expression Matching]]></title>
    <url>%2F2019%2F05%2F26%2FRegular-Expression-Matching%2F</url>
    <content type="text"><![CDATA[问题描述 字符匹配问题 用p来匹配s. p中可能包含. * . 表示匹配任一个单一字符 * 表示匹配0到多个前一个字符 要求p和s匹配 Example 1: Input: &gt; s=&quot;aa&quot; &gt; p = &quot;a&quot; Output: false Explanation: &gt; &quot;a&quot; does not match the entire string &quot;aa&quot; Example 2: Input: &gt; s=&quot;aa&quot; &gt; p = &quot;a*&quot; Output: true Explanation: &gt; &quot;'*' means zero or more of the precedeng element, 'a'. Therefore, by repeating 'a' once, it becomes &quot;aa&quot; Example 3: Input: &gt; s=&quot;aab&quot; &gt; p = &quot;c*a*b&quot; Output: true Explanation: &gt; c can be repeated 0 times, a can be repeated 1 time. Therefore it matches &quot;aab&quot;. 思路 这是一道非常困难的题, 曾经害我苦思冥想了好几天。我刚开始甚至没想到居然能用DP解这个问题，后来苦苦求索翻阅leetcode一众大神的解释后才搞明白这个题用DP到底怎么搞 这是一道true false dp问题, dp[i][j] 表示了s前i个元素和p前j个元素的匹配情况 分情况讨论： 如果s[i] == p [j] || p[j] == '.' 这时候完全就看前面的情况 dp[i][j] = dp[i-1][j-1] 如果s[i] != p [j]这个也要分情况 2.1 如果p[j] == '*' (ba a* ab a*) 2.1.1 若 p[j-1] != s[i] &amp;&amp; p[j-1] != '.' 此时, 由于上一个元素不匹配导致*无法复制，那么*只能让上一个元素清空: &gt; dp[i][j] = dp[i][j-2] 2.1.2 除上面的情况外，即可以复制上一个元素达到匹配的目的，当然也可以不复制上一个元素: &gt; dp[i][j] = dp[i][j-2] (上一个元素清空) || dp[i][j-1] (*只代表一个元素) || dp[i-1][j] (代表多个元素，如果在i前面都能和p匹配，那加一个自然也能匹配*) 2.2 如果p[j] != '*' &gt; dp[i][j] = false 特别解释一下，为什么在2.1.2中，当代表多个元素时，匹配的是dp[i-1][j]这个奇怪的搭配。让我们来举个栗子: 假设我们的有 &gt; s: abbbbb &gt; p: cb* 我们看到这里的*，实际上是代表了5个b,但是当我们求dp[i][j]的时候，我们无法得匹配完这些b之后前面的元素是否匹配，我们删掉这些b &gt; s: a &gt; p: c 也就是说，a,c是否匹配已经在之前迭代了。如何得知a,c的迭代位置呢？ &gt; dp[i][j] = dp[i-1][j] = …… = dp[i-6][j] 这是通过我们之前已经求到的结果迭代出来的，你会发现i递减的过程其实就是在找重复元素之前的元素，所以我们直接给出了dp[i-1][j] 标准代码 其实如果能看懂上面的解释的话，代码不成问题，上面的解释已经接近于伪代码了。 12345678910111213141516171819202122public boolean isMatch(String s, String p) &#123; boolean[][] dp = new boolean[s.length()+1][p.length()+1]; // s为空, p为空 dp[0][0] = true; // s为空， p不空 for (int j = 1; j &lt;= p.length(); j++) &#123; if (p.charAt(j-1)=='*' &amp;&amp; dp[0][j-2]) dp[0][j] = true; &#125; //s不空，p空，直接默认false for (int i = 1; i &lt; s.length() + 1; i++) &#123; for (int j = 1; j &lt; p.length() + 1; j++) &#123; if (s.charAt(i-1)==p.charAt(j-1) || p.charAt(j-1)=='.') dp[i][j] = dp[i-1][j-1]; else &#123; if (p.charAt(j-1)=='*')&#123; if (p.charAt(j-2)!=s.charAt(i-1)&amp;&amp;p.charAt(j-2)!='.') dp[i][j] = dp[i][j-2]; else dp[i][j] = dp[i][j-2] || dp[i][j-1] || dp[i-1][j]; &#125; &#125; &#125; &#125; return dp[s.length()][p.length()]; &#125; 终于啃完这个头疼的问题了，看张图片奖励下自己吧]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClimbingStairs]]></title>
    <url>%2F2019%2F05%2F25%2FClimbingStairs%2F</url>
    <content type="text"><![CDATA[问题描述 这道题是easy题： You are climbing a stair case. It takes n steps to reach to the top. Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top? 1. example1: Input: 2 Output: 2 Explanation: 1. 1 step + 1 step 2. 2 steps 2. example2: Input: 3 Output: 3 Explanation: 1. 1 step + 1 step + 1 step 2. 1 step + 2 steps 3. 2 steps + 1 step 思路 这个就很简单了, 典型的计数型动态规划问题，常规做法： 记dp[n] 表示到高度n有dp[n]种方法，则dp[n] = dp[n-1]+dp[n-2] 边界： dp[0] = 1 dp[1] = 2 计算顺序：从左到右 代码 1234567891011public int climbStairs(int n) &#123; int[] dp = new int[n]; dp[0] = 1; if (n&gt;1) dp[1] = 2; if (n&gt;2)&#123; for (int i = 2; i &lt; n; i++) &#123; dp[i] = dp[i-1]+dp[i-2]; &#125; &#125; return dp[n-1]; &#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[孔雀东南飞（中文测试）]]></title>
    <url>%2F2019%2F05%2F24%2F%E4%B8%AD%E6%96%87%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[孔雀东南飞插图 汉末建安中，庐江府小吏焦仲卿妻刘氏，为仲卿母所遣，自誓不嫁。其家逼之，乃投水而死。仲卿闻之，亦自缢于庭树。时人伤之，为诗云尔。 孔雀东南飞，五里一徘徊。 十三能织素6，十四学裁衣，十五弹箜篌7，十六诵诗书8。十七为君妇，心中常苦悲。君既为府吏，守节9情不移。贱妾留空房，相见常日稀。鸡鸣入机织，夜夜不得息。三日断10五匹，大人故嫌迟11。非为织作迟，君家妇难为！妾不堪12驱使，徒13留无所施14。便可白公姥15，及时相遣归 府吏得闻之，堂上启阿母：“儿已薄禄相16，幸复得此妇，结发17同枕席，黄泉共为友。共事二三年，始尔18未为久。女行无偏斜，何意致不厚19？” 阿母谓府吏：“何乃太区区20！此妇无礼节，举动自专由21。吾意久怀忿，汝岂得自由！东家有贤22女，自名秦罗敷，可怜23体无比，阿母为汝求。便可速遣之，遣去慎莫留！” 府吏长跪告：“伏惟24启阿母，今若遣此妇，终老不复取25！” 阿母得闻之，槌床26便大怒：“小子无所畏，何敢助妇语！吾已失恩义，会不相从许27！” 府吏默无声，再拜还入户。举言28谓新妇29，哽咽不能语：“我自不驱卿，逼迫有阿母。卿但暂还家，吾今且报府30。不久当归还，还必相迎取。以此下心意31，慎勿违吾语。” 新妇谓府吏：“勿复重纷纭32。往昔初阳岁33，谢34家来贵门。奉事循公姥，进止敢自专？昼夜勤作息35，伶俜萦苦辛36。谓言37无罪过，供养卒38大恩；仍更被驱遣，何言复来还！妾有绣腰襦39，葳蕤40自生光；红罗复斗帐，四角垂香囊；箱帘41六七十，绿碧青丝绳，物物各自异，种种在其中。人贱物亦鄙，不足迎后人42，留待作遗施43，于今无会因44。时时为安慰，久久莫相忘！” 鸡鸣外欲曙，新妇起严妆45。著我绣夹裙，事事四五通46。足下蹑47丝履，头上玳（dài）瑁（mào）48光。腰若流纨素，耳著明月珰49。指如削葱根，口如含朱丹。纤纤作细步，精妙世无双。 上堂拜阿母，阿母怒不止。“昔作女儿时50，生小出野里51。本自无教训，兼愧52贵家子。受母钱帛多，不堪母驱使。今日还家去，念母劳家里。”却53与小姑别，泪落连珠子。“新妇初来时，小姑始扶床；今日被驱遣，小姑如我长。勤心养公姥，好自相扶将54。初七及下九55，嬉戏莫相忘。”出门登车去，涕落百余行。 府吏马在前，新妇车在后。隐隐56何甸甸，俱会大道口。下马入车中，低头共耳语：“誓不相隔卿，且暂还家去；吾今且赴府，不久当还归。誓天不相负！”57 新妇谓府吏：“感君区区58怀！君既若见录59，不久望君来。君当作磐石，妾当作蒲苇，蒲苇纫60如丝，磐石无转移。我有亲父兄61，性行暴如雷，恐不任我意，逆62以煎我怀。”举手长劳劳63，二情同依依。 入门上家堂，进退无颜仪64。阿母大拊掌65，不图子自归66：“十三教汝织，十四能裁衣，十五弹箜篌，十六知礼仪，十七遣汝嫁，谓言无誓违67。汝今何罪过，不迎而自归？”兰芝惭阿母：“儿实无罪过。”阿母大悲摧68。 还家十余日，县令遣媒来。云有第三郎，窈窕69世无双。年始十八九，便言多令才70。 阿母谓阿女：“汝可去应之。” 阿女含泪答：“兰芝初还时，府吏见丁宁71，结誓不别离。今日违情义，恐此事非奇72。自可断来信73，徐徐更谓之74。” 阿母白媒人：“贫贱有此女，始适75还家门。不堪76吏人妇，岂合令郎君？幸可广问讯，不得便相许。” 媒人去数日，寻遣丞请还，说有兰家女，丞籍有宦官77。云有第五郎，娇逸78未有婚。遣丞为媒人，主簿79通语言。直说太守家，有此令郎君，既欲结大义，故遣来贵门。 阿母谢媒人：“女子先有誓，老姥岂敢言！” 阿兄得闻之，怅然心中烦。举言谓阿妹：“作计80何不量81！先嫁得府吏，后嫁得郎君，否泰82如天地，足以荣汝身。不嫁义郎83体，其往欲何云84？” 兰芝仰头答：“理实如兄言。谢家事夫婿，中道还兄门。处分85适86兄意，那得自任专！虽与府吏要87，渠会88永无缘。登即89相许和，便可作婚姻。“ 媒人下床去，诺诺复尔尔90。还部白府君91：“下官92奉使命，言谈大有缘93。”府君得闻之，心中大欢喜。视历94复开书，便利此月内，六合95正相应。良吉三十日，今已二十七，卿96可去成婚。交语97速装束，络绎如浮云。青雀白鹄舫98，四角龙子幡99。婀娜100随风转，金车玉作轮。踯躅101青骢马102，流苏103金镂鞍。赍104钱三百万，皆用青丝穿。杂彩105三百匹，交广106市鲑107珍。从人四五百，郁郁108登郡门。 阿母谓阿女：“适109得府君书，明日来迎汝。何不作衣裳？莫令事不举110！” 阿女默无声，手巾掩口啼，泪落便如泻。移我琉璃榻111，出置前窗下。左手持刀尺，右手执绫罗。朝成绣夹裙，晚成单罗衫。晻晻112日欲暝，愁思出门啼。 府吏闻此变，因求假暂归。未至二三里，摧藏113马悲哀。新妇识马声，蹑履相逢迎。怅然遥相望，知是故人来。举手拍马鞍，嗟叹使心伤：“自君别我后，人事不可量114。果不如先愿，又非君所详。我有亲父母115，逼迫兼弟兄116。以我应他人，君还何所望！” 府吏谓新妇：“贺卿得高迁！磐石方且厚，可以卒千年；蒲苇一时纫，便作旦夕间。卿当日胜贵117，吾独向黄泉！” 新妇谓府吏：“何意出此言！同是被逼迫，君尔妾亦然。黄泉下相见，勿违今日言！”执手分道去，各各还家门。生人作死别，恨恨118那可论？念与世间辞，千万不复全！ 府吏还家去，上堂拜阿母：“今日大风寒，寒风摧树木，严霜结庭兰。儿今日冥冥119，令母在后单120。故121作不良计122，勿复怨鬼神！命如南山石，四体123康且直124！” 阿母得闻之，零泪应声落：“汝是大家子，仕宦于台阁125。慎勿为妇死，贵贱情何薄126！东家有贤女，窈窕艳城郭，阿母为汝求，便复在旦夕。” 府吏再拜还，长叹空房中，作计乃尔立127。转头向户里，渐见愁煎迫。 其日牛马嘶，新妇入青庐128。奄奄129黄昏130后，寂寂人定初。“我命绝今日，魂去尸长留！”揽裙脱丝履，举身赴清池。 府吏闻此事，心知长别离。徘徊庭树下，自挂东南枝。 两家求合葬，合葬华山131傍。东西植松柏，左右种梧桐。枝枝相覆盖，叶叶相交通132。中有双飞鸟，自名为鸳鸯。仰头相向鸣，夜夜达五更。行人驻足133听，寡妇起彷徨。多谢134后世人，戒之慎勿忘！ [10]]]></content>
      <categories>
        <category>文言文</category>
      </categories>
      <tags>
        <tag>文学</tag>
      </tags>
  </entry>
</search>
